{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandini2595/Stanford_Hackathon_AI_ArgumentCounter/blob/main/Solution/Lawgorithms_StanfordHackathon_HighlightTokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_CkzXx-Zcbn"
      },
      "outputs": [],
      "source": [
        "# üß† Install dependencies\n",
        "\n",
        "!pip install -q sentence-transformers gradio transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Sentence-level encoder for matching\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Token-level model for token embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "base_model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "i4x-_Q9bZk6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brief_pairs = []\n",
        "moving_brief_index = {}\n",
        "\n",
        "def combine_argument_text(arg):\n",
        "    return arg[\"heading\"] + \". \" + arg[\"content\"]\n",
        "\n",
        "def get_full_texts(pair):\n",
        "    full_moving = \" \".join([combine_argument_text(arg) for arg in pair[\"moving_brief\"][\"brief_arguments\"]])\n",
        "    full_response = \" \".join([combine_argument_text(arg) for arg in pair[\"response_brief\"][\"brief_arguments\"]])\n",
        "    return full_moving, full_response\n"
      ],
      "metadata": {
        "id": "q3--XxYSZmnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_file_upload(file_obj):\n",
        "    global brief_pairs, moving_brief_index\n",
        "    brief_pairs = json.load(open(file_obj.name))\n",
        "\n",
        "    moving_brief_index = {}\n",
        "    for pair in brief_pairs:\n",
        "        brief_id = pair[\"moving_brief\"][\"brief_id\"]\n",
        "        if brief_id not in moving_brief_index:\n",
        "            moving_brief_index[brief_id] = {\n",
        "                \"pair\": pair,\n",
        "                \"headings\": [arg[\"heading\"] for arg in pair[\"moving_brief\"][\"brief_arguments\"]]\n",
        "            }\n",
        "\n",
        "    return gr.update(choices=list(moving_brief_index.keys()), value=None), gr.update(choices=[], value=None), \"\", \"\", \"\"\n"
      ],
      "metadata": {
        "id": "NkWwBOZSZsai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_headings_for_brief(brief_id):\n",
        "    if brief_id not in moving_brief_index:\n",
        "        return gr.update(choices=[], value=None)\n",
        "    return gr.update(choices=moving_brief_index[brief_id][\"headings\"], value=None)"
      ],
      "metadata": {
        "id": "3uFDiEUsZvlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_embeddings(text):\n",
        "    tokens = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        output = base_model(**tokens)\n",
        "    return output.last_hidden_state.squeeze(0), tokens[\"input_ids\"].squeeze(0)\n",
        "\n",
        "def get_top_contributing_words(moving_text, response_text, top_n=5):\n",
        "    try:\n",
        "        mov_emb, mov_ids = get_token_embeddings(moving_text)\n",
        "        res_emb, res_ids = get_token_embeddings(response_text)\n",
        "        sim_matrix = util.pytorch_cos_sim(mov_emb, res_emb).cpu().numpy()\n",
        "        top_scores = np.max(sim_matrix, axis=1)\n",
        "        top_indices = top_scores.argsort()[::-1][:top_n]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(mov_ids)\n",
        "        return [tokens[i] for i in top_indices if tokens[i] not in tokenizer.all_special_tokens]\n",
        "    except Exception as e:\n",
        "        print(\"üî¥ Error in get_top_contributing_words():\", e)\n",
        "        return [\"‚ùå Token match error\"]\n",
        "\n",
        "def highlight_top_tokens_in_text(text, top_tokens):\n",
        "    for tok in top_tokens:\n",
        "        if tok.startswith(\"##\"): tok = tok[2:]\n",
        "        if len(tok) > 2 and tok.lower() in text.lower():\n",
        "            text = text.replace(tok, f\"<mark style='background-color: #ffff99'>{tok}</mark>\")\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "RImMdf8Sf1FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_html_table(data):\n",
        "    if not data or len(data) < 2:\n",
        "        return \"<p>No results.</p>\"\n",
        "\n",
        "    html = \"<div style='overflow-x:auto'><table style='width:100%; border-collapse: collapse;'>\"\n",
        "    html += \"<thead><tr>\"\n",
        "    for col in data[0]:\n",
        "        html += f\"<th style='border: 1px solid #ccc; padding: 8px; background-color: #f0f0f0; text-align: left'>{col}</th>\"\n",
        "    html += \"</tr></thead><tbody>\"\n",
        "\n",
        "    for row in data[1:]:\n",
        "        html += \"<tr>\"\n",
        "        for cell in row:\n",
        "            html += f\"<td style='border: 1px solid #ddd; padding: 8px; vertical-align: top; word-break: break-word'>{cell}</td>\"\n",
        "        html += \"</tr>\"\n",
        "\n",
        "    html += \"</tbody></table></div>\"\n",
        "    return html\n"
      ],
      "metadata": {
        "id": "hNjlNDLomGfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def match_counter_arguments(brief_id, selected_heading, top_k=5):\n",
        "    if brief_id not in moving_brief_index:\n",
        "        return \"<p>Error: Brief not found.</p>\", \"\", \"\"\n",
        "\n",
        "    pair = moving_brief_index[brief_id][\"pair\"]\n",
        "    moving_args = pair[\"moving_brief\"][\"brief_arguments\"]\n",
        "    response_args = pair[\"response_brief\"][\"brief_arguments\"]\n",
        "\n",
        "    moving_index = next((i for i, arg in enumerate(moving_args) if arg[\"heading\"] == selected_heading), None)\n",
        "    if moving_index is None:\n",
        "        return \"<p>Error: Heading not found.</p>\", \"\", \"\"\n",
        "\n",
        "    moving_text = combine_argument_text(moving_args[moving_index]).strip()\n",
        "    response_texts = [combine_argument_text(arg).strip() for arg in response_args]\n",
        "\n",
        "    try:\n",
        "        moving_emb = model.encode([moving_text], convert_to_tensor=True)\n",
        "        response_emb = model.encode(response_texts, convert_to_tensor=True)\n",
        "        sim_scores = util.pytorch_cos_sim(moving_emb, response_emb)[0].cpu().numpy()\n",
        "    except Exception as e:\n",
        "        return f\"<p>Error computing similarity: {e}</p>\", \"\", \"\"\n",
        "\n",
        "    top_indices = sim_scores.argsort()[::-1][:top_k]\n",
        "\n",
        "    result_table = [[\"#\", \"Response Brief ID\", \"Heading\", \"Match %\", \"Excerpt\", \"Top Tokens\"]]\n",
        "\n",
        "    try:\n",
        "        full_moving_text, full_response_text = get_full_texts(pair)\n",
        "        # full_moving_text = full_moving_text[:2000]\n",
        "        # full_response_text = full_response_text[:2000]\n",
        "        top_tokens = get_top_contributing_words(full_moving_text, full_response_text)\n",
        "        highlighted_moving = highlight_top_tokens_in_text(full_moving_text, top_tokens)\n",
        "        highlighted_response = highlight_top_tokens_in_text(full_response_text, top_tokens)\n",
        "    except:\n",
        "        highlighted_moving = \"<i>Error in highlight</i>\"\n",
        "        highlighted_response = \"<i>Error in highlight</i>\"\n",
        "        top_tokens = [\"‚ùå\"]\n",
        "\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        resp_arg = response_args[idx]\n",
        "        excerpt = resp_arg[\"content\"].replace(\"\\n\", \" \").strip()[:300] + \"...\"\n",
        "        result_table.append([\n",
        "            str(i + 1),\n",
        "            pair[\"response_brief\"][\"brief_id\"],\n",
        "            resp_arg[\"heading\"],\n",
        "            f\"{sim_scores[idx]*100:.2f}%\",\n",
        "            excerpt,\n",
        "            \", \".join(top_tokens)\n",
        "        ])\n",
        "\n",
        "    return render_html_table(result_table), highlighted_moving, highlighted_response\n"
      ],
      "metadata": {
        "id": "kZAe85dbjt3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## ‚öñÔ∏è Upload & Match Legal Brief Arguments with Explainability\")\n",
        "    gr.Markdown(\"Upload `brief_pairs.json`, select a moving brief and heading. View matched counter-arguments with <mark>highlighted top tokens</mark>.\")\n",
        "\n",
        "    file_upload = gr.File(label=\"üìÅ Upload `brief_pairs.json`\")\n",
        "\n",
        "    with gr.Row():\n",
        "        dropdown_brief = gr.Dropdown(choices=[], label=\"üìÇ Select Moving Brief ID\")\n",
        "        dropdown_heading = gr.Dropdown(choices=[], label=\"üìò Select Argument Heading\")\n",
        "\n",
        "    output_table_html = gr.HTML(label=\"üìä Top Counter-Argument Matches\")\n",
        "    highlighted_moving_md = gr.HTML(label=\"üßæ Moving Brief (Highlighted)\")\n",
        "    highlighted_response_md = gr.HTML(label=\"üìï Response Brief (Highlighted)\")\n",
        "\n",
        "    file_upload.change(fn=handle_file_upload, inputs=file_upload,\n",
        "                       outputs=[dropdown_brief, dropdown_heading, output_table_html, highlighted_moving_md, highlighted_response_md])\n",
        "\n",
        "    dropdown_brief.change(fn=get_headings_for_brief, inputs=dropdown_brief, outputs=dropdown_heading)\n",
        "\n",
        "    dropdown_heading.change(fn=match_counter_arguments,\n",
        "                            inputs=[dropdown_brief, dropdown_heading],\n",
        "                            outputs=[output_table_html, highlighted_moving_md, highlighted_response_md])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXn9T4ctf8Gq",
        "outputId": "0fb8753c-0e13-433a-eab9-6f18d761524b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2db0518797b901b325.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2db0518797b901b325.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}